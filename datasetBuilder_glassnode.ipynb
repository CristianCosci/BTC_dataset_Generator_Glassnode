{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datasetBuilder_glassnode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaHD8tL5p+rBgfKHLF/uQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristianCosci/BTC_dataset_Generator_glassnode/blob/main/datasetBuilder_glassnode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fk7XyvKGENu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "\n",
        "connected = False\n",
        "\n",
        "if(not connected):\n",
        "  drive.mount('/content/drive', force_remount= True)\n",
        "  path = '/content/drive/MyDrive/progettoBTC/'\n",
        "  connected = True\n",
        "  f = open(\"{}secret.txt\".format(path))\n",
        "  API_KEY = f.read().replace(\"\\n\", \"\")\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(interval, since, dataset_type, eth_data=False):\n",
        "  query_total = [] # To store an array of dict, each for every query\n",
        "  with open(path+dataset_type,'r') as file:\n",
        "    # reading each line\n",
        "    for line in file:\n",
        "        count = 0\n",
        "        # print(line)\n",
        "        link = line.split()[0] # The first element of the line is the link for the query\n",
        "        query = {'link': link} # Put in a dict\n",
        "        labels = [] # To store the word of the line\n",
        "        for word in line.split()[1:]: # Split the word of the line\n",
        "            labels.append(word)\n",
        "            count += 1\n",
        "        for i in range(0, count, 2): # Step of 2 on reading, beacause need couple (name in new df, name returned by query)\n",
        "          query[labels[i]] = labels[i+1]\n",
        "          # print(query)\n",
        "        query_total.append(query)\n",
        "        query = {} # Reset dict\n",
        "  \n",
        "  df = pd.DataFrame()\n",
        "  for i in query_total:\n",
        "    link = i.pop('link')\n",
        "    # print(link)\n",
        "    res = requests.get(link,\n",
        "    params={'a': 'BTC','s': since,'i': interval, 'api_key': API_KEY})\n",
        "    foo_df = pd.json_normalize(json.loads(res.text))\n",
        "    foo_df[\"datetime\"] = pd.to_datetime(foo_df[\"t\"], unit=\"s\") #timestamp conversion to datetime\n",
        "    foo_df = foo_df.drop(\"t\", axis=1).set_index(\"datetime\").sort_index()\n",
        "    for j in i.keys(): # To add a new column in the real dataset -> using couple (name in new df, name returned by query)\n",
        "      print(j)\n",
        "      df[j] = foo_df[i[j]]\n",
        "  \n",
        "  if eth_data: # If requested download also eth data\n",
        "    if dataset_type == 'hourly_data.txt':\n",
        "      df = download_dataset_eth(df, interval = interval , since = since, dataset_type = 'hourly_data_eth.txt')\n",
        "    elif dataset_type == 'daily_data.txt':\n",
        "      download_dataset_eth(df, interval = interval , since = since, dataset_type = 'hourly_data_eth.txt')\n",
        "  return df\n",
        "\n",
        "\n",
        "def download_dataset_eth(df, interval , since, dataset_type):\n",
        "  query_total = [] # To store an array of dict, each for every query\n",
        "  with open(path+dataset_type,'r') as file:\n",
        "    # reading each line\n",
        "    for line in file:\n",
        "        count = 0\n",
        "        # print(line)\n",
        "        link = line.split()[0] # The first element of the line is the link for the query\n",
        "        query = {'link': link} # Put in a dict\n",
        "        labels = [] # To store the word of the line\n",
        "        for word in line.split()[1:]: # Split the word of the line\n",
        "            labels.append(word)\n",
        "            count += 1\n",
        "        for i in range(0, count, 2): # Step of 2 on reading, beacause need couple (name in new df, name returned by query)\n",
        "          query[labels[i]] = labels[i+1]\n",
        "          # print(query)\n",
        "        query_total.append(query)\n",
        "        query = {} # Reset dict\n",
        "  \n",
        "  df = pd.DataFrame()\n",
        "  for i in query_total:\n",
        "    link = i.pop('link')\n",
        "    res = requests.get(link,\n",
        "    params={'a': 'ETH','s': since,'i': interval, 'api_key': API_KEY})\n",
        "    foo_df = pd.json_normalize(json.loads(res.text))\n",
        "    foo_df[\"datetime\"] = pd.to_datetime(foo_df[\"t\"], unit=\"s\") #timestamp conversion to datetime\n",
        "    foo_df = foo_df.drop(\"t\", axis=1).set_index(\"datetime\").sort_index()\n",
        "    for j in i.keys(): # To add a new column in the real dataset -> using couple (name in new df, name returned by query)\n",
        "      print(j)\n",
        "      df[j] = foo_df[i[j]]\n",
        "  return df"
      ],
      "metadata": {
        "id": "MERopuTpKbA9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hourly = download_dataset(interval = '1h', since = '1356998400', dataset_type = 'hourly_data.txt', eth_data = True)\n",
        "# df_daily = download_dataset(interval = '24h', since = '1356998400', dataset_type = 'daily_data.txt', eth_data = True)"
      ],
      "metadata": {
        "id": "pXTHwnMsK1G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hourly.to_csv(path+\"BTC_hourly_dataset.csv\")\n",
        "# df_daily.to_csv(path+\"BTC_daily_dataset.csv\")"
      ],
      "metadata": {
        "id": "dY_r33dQLYrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hourly"
      ],
      "metadata": {
        "id": "_EZ0uDsM9h8a"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR DEBUG\n",
        "res = requests.get('https://api.glassnode.com/v1/metrics/protocols/uniswap_volume_sum' ,\n",
        "    params={'a': 'ETH','s': '1356998400','i': '1h', 'api_key': API_KEY})\n",
        "foo_df = pd.json_normalize(json.loads(res.text))\n",
        "print(foo_df)\n",
        "foo_df[\"datetime\"] = pd.to_datetime(foo_df[\"t\"], unit=\"s\") #timestamp conversion to datetime\n",
        "foo_df = foo_df.drop(\"t\", axis=1).set_index(\"datetime\").sort_index()"
      ],
      "metadata": {
        "id": "05OQtfphPkNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foo_df"
      ],
      "metadata": {
        "id": "ny8KUt0oG7hL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}